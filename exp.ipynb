{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/arihanth.srikar/miniconda3/envs/pytorch2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from mlm_pytorch.mlm_pytorch.mlm_pytorch import MLM\n",
    "from x_transformers.x_transformers import TransformerWrapper, Encoder, Decoder\n",
    "from x_transformers.autoregressive_wrapper import AutoregressiveWrapper\n",
    "\n",
    "import codecs\n",
    "from SmilesPE.pretokenizer import atomwise_tokenizer\n",
    "from SmilesPE.tokenizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class USPTOIFT(Dataset):\n",
    "    \n",
    "    def __init__(self, data_dir: str='data/uspto_IFT', split: str='val', to_gen: int=-1, extra: str='') -> None:\n",
    "\n",
    "        # target is the reactant\n",
    "        with open(f'{data_dir}/{split}/reactants.txt', 'r') as f:\n",
    "            self.reactants = f.read().splitlines()\n",
    "        self.reactants = [r.split(' ') for r in self.reactants]\n",
    "\n",
    "        # source or input is the product\n",
    "        with open(f'{data_dir}/{split}/products.txt', 'r') as f:\n",
    "            self.products = f.read().splitlines()\n",
    "        self.products = [p.split(' ') for p in self.products]\n",
    "\n",
    "        # verify that the dataset is consistent\n",
    "        assert len(self.reactants) == len(self.products), 'Mismatched length of reactants and products'\n",
    "        self.to_gen = to_gen if to_gen > 0 else len(self.reactants)\n",
    "\n",
    "        # vocab and tokenizer\n",
    "        with open(f'{data_dir}/vocab{extra}.txt', 'r') as f:\n",
    "            self.token_decoder = f.read().splitlines()\n",
    "        self.token_encoder = {t: i for i, t in enumerate(self.token_decoder)}\n",
    "\n",
    "        # sanity check the tokenizer\n",
    "        print(f'Performing sanity check on vocab and tokenizer...')\n",
    "        reactant_set = set(chain.from_iterable(self.reactants))\n",
    "        product_set = set(chain.from_iterable(self.products))\n",
    "        all_chars = reactant_set.union(product_set)\n",
    "        assert all_chars <= set(self.token_encoder.keys()), \"Tokenizer is not consistent with the dataset\"\n",
    "\n",
    "        # additional information\n",
    "        self.vocab_size = len(self.token_decoder)\n",
    "        self.pad_token_id = self.token_encoder['<pad>']\n",
    "        self.mask_token_id = self.token_encoder['<mask>']\n",
    "        self.mask_ignore_token_ids = [v for k, v in self.token_encoder.items() if '<' in k and '>' in k]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.to_gen\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        r, p = self.reactants[idx], self.products[idx]\n",
    "        num_reactants, num_products = r.count('.')+1, p.count('.')+1\n",
    "\n",
    "        r = [f'<{num_reactants}>'] + ['<sos>'] + r + ['<eos>']\n",
    "        p = [f'<{num_products}>']  + ['<sos>'] + p + ['<eos>']\n",
    "        \n",
    "        r = [self.token_encoder[t] for t in r]\n",
    "        p = [self.token_encoder[t] for t in p]\n",
    "\n",
    "        src_mask = [True] * len(p)\n",
    "\n",
    "        r, p, src_mask = torch.tensor(r), torch.tensor(p), torch.tensor(src_mask).bool()\n",
    "\n",
    "        return r, p, src_mask\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "\n",
    "        # unpack the input data\n",
    "        r, p, src_mask = zip(*data)\n",
    "        \n",
    "        # pad the encoder stuff\n",
    "        p = pad_sequence(p, batch_first=True, padding_value=self.pad_token_id)\n",
    "        src_mask = pad_sequence(src_mask, batch_first=True, padding_value=False).bool()\n",
    "        \n",
    "        # pad the decoder stuff\n",
    "        r = pad_sequence(r, batch_first=True, padding_value=self.pad_token_id)\n",
    "        \n",
    "        return r, p, src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing sanity check on vocab and tokenizer...\n"
     ]
    }
   ],
   "source": [
    "val_dataset = USPTOIFT(split='train', to_gen=-1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=val_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p, src_mask = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 80]), torch.Size([32, 60]), torch.Size([32, 60]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape, p.shape, src_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levy.levenshteinaugment.levenshtein import Levenshtein_augment\n",
    "\n",
    "#Supress warnings from RDKit\n",
    "from rdkit import rdBase\n",
    "rdBase.DisableLog('rdApp.error')\n",
    "rdBase.DisableLog('rdApp.warning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reactants_mol</th>\n",
       "      <th>products_mol</th>\n",
       "      <th>reaction_type</th>\n",
       "      <th>set</th>\n",
       "      <th>num_reacts</th>\n",
       "      <th>num_prods</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CS(=O)(=O)OC[C@H]1CCC(=O)O1, Fc1ccc(Nc2ncnc3c...</td>\n",
       "      <td>[O=C1CC[C@H](CN2CCN(CCOc3cc4ncnc(Nc5ccc(F)c(Cl...</td>\n",
       "      <td>&lt;RX_1&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.121951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[COC(=O)c1cc(CCCc2cc3c(=O)[nH]c(N)nc3[nH]2)cs1]</td>\n",
       "      <td>[Nc1nc2[nH]c(CCCc3csc(C(=O)O)c3)cc2c(=O)[nH]1]</td>\n",
       "      <td>&lt;RX_6&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CC1(C)OB(B2OC(C)(C)C(C)(C)O2)OC1(C)C, FC(F)(F...</td>\n",
       "      <td>[CC1(C)OB(c2cccc(Nc3nccc(C(F)(F)F)n3)c2)OC1(C)C]</td>\n",
       "      <td>&lt;RX_9&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)OCc1ccccc1]</td>\n",
       "      <td>[CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)O]</td>\n",
       "      <td>&lt;RX_6&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.318182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Fc1cc2c(Cl)ncnc2cn1, NC1CCCCCC1]</td>\n",
       "      <td>[Fc1cc2c(NC3CCCCCC3)ncnc2cn1]</td>\n",
       "      <td>&lt;RX_1&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.052632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       reactants_mol  \\\n",
       "0  [CS(=O)(=O)OC[C@H]1CCC(=O)O1, Fc1ccc(Nc2ncnc3c...   \n",
       "1    [COC(=O)c1cc(CCCc2cc3c(=O)[nH]c(N)nc3[nH]2)cs1]   \n",
       "2  [CC1(C)OB(B2OC(C)(C)C(C)(C)O2)OC1(C)C, FC(F)(F...   \n",
       "3  [CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)OCc1ccccc1]   \n",
       "4                  [Fc1cc2c(Cl)ncnc2cn1, NC1CCCCCC1]   \n",
       "\n",
       "                                        products_mol reaction_type    set  \\\n",
       "0  [O=C1CC[C@H](CN2CCN(CCOc3cc4ncnc(Nc5ccc(F)c(Cl...        <RX_1>  train   \n",
       "1     [Nc1nc2[nH]c(CCCc3csc(C(=O)O)c3)cc2c(=O)[nH]1]        <RX_6>  train   \n",
       "2   [CC1(C)OB(c2cccc(Nc3nccc(C(F)(F)F)n3)c2)OC1(C)C]        <RX_9>  train   \n",
       "3           [CC(C)(C)OC(=O)NCC(=O)CCC(=O)OCCCC(=O)O]        <RX_6>  train   \n",
       "4                      [Fc1cc2c(NC3CCCCCC3)ncnc2cn1]        <RX_1>  train   \n",
       "\n",
       "   num_reacts  num_prods     ratio  \n",
       "0           2          1  1.121951  \n",
       "1           1          1  1.045455  \n",
       "2           2          1  1.384615  \n",
       "3           1          1  1.318182  \n",
       "4           2          1  1.052632  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('data/uspto50/processed.pickle')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = Levenshtein_augment(source_augmentation=2, randomization_tries=1000)\n",
    "\n",
    "def augment(reactants, products):\n",
    "    reactants = sorted(reactants, key=lambda x: len(x), reverse=True)\n",
    "    products  = sorted(products, key=lambda x: len(x), reverse=True)\n",
    "    \n",
    "    new_reactants, new_products, all_score = [], [], []\n",
    "    for i in range(1, len(reactants)+1):\n",
    "        reactant, product = '.'.join(reactants[:i]), '.'.join(products[:i])\n",
    "    \n",
    "        pairs = augmenter.levenshtein_pairing(reactant, product)\n",
    "        augmentations = augmenter.sample_pairs(pairs)\n",
    "    \n",
    "        for new_reactant, new_product, score in augmentations:\n",
    "            new_reactants.append(new_reactant)\n",
    "            new_products.append(new_product)\n",
    "            all_score.append(score)\n",
    "    \n",
    "    return new_reactants, new_products, all_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50037 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/50037 [00:03<17:37:03,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "# random_idx = np.random.randint(0, len(df))\n",
    "# reactants, products = df['reactants_mol'].iloc[random_idx], df['products_mol'].iloc[random_idx]\n",
    "\n",
    "new_df = df[df['reactants_mol'].apply(len) >= 3]\n",
    "\n",
    "tok_reactants = []\n",
    "tok_products  = []\n",
    "\n",
    "# for i, (reactants, products) in enumerate(tqdm(zip(new_df['reactants_mol'], new_df['products_mol']), total=len(new_df))):\n",
    "for i, (reactants, products) in enumerate(tqdm(zip(df['reactants_mol'], df['products_mol']), total=len(df))):\n",
    "    new_reactants, new_products, score = augment(reactants, products)\n",
    "    # print(f'{\".\".join(reactants)} -> {\".\".join(products)}')\n",
    "    for reactant, product, sc in zip(new_reactants, new_products, score):\n",
    "        # print(f'{sc:.2f}: {reactant} -> {product}')\n",
    "        tok_reactants.append(atomwise_tokenizer(reactant))\n",
    "        tok_products.append(atomwise_tokenizer(product))\n",
    "    # print()\n",
    "    if i >= 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O 1 C ( C ) ( C ) C ( C ) ( C ) O B 1 B 1 O C ( C ) ( C ) C ( C ) ( C ) O 1 -> O 1 C ( C ) ( C ) C ( C ) ( C ) O B 1 c 1 c c c c ( N c 2 n c c c ( C ( F ) ( F ) F ) n 2 ) c 1\n"
     ]
    }
   ],
   "source": [
    "rand_idx = 6\n",
    "print(f\"{' '.join(tok_reactants[rand_idx])} -> {' '.join(tok_products[rand_idx])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145 ['C', 'S', '(', '=', 'O', ')', '[C@H]', '1', '.', 'F', 'c', 'N', '2', 'n', '3', '4', 'Cl', '[nH]', 's', 'B', 'Br', '[N+]', '[O-]', '[C@@H]', '-', '5', 'o', '/', '[Li]', '[N-]', '#', '[C@@]', '[Si]', 'I', 'P', '[Mg+]', '[P+]', '[S-]', '[Se]', '[C@]', '\\\\', '[Sn]', '[NH4+]', '[SiH2]', '[NH3+]', '[K]', '[SiH]', '[Zn+]', '6', '[C-]', '[Cu]', '[n+]', '[S@@]', '[PH]', '[se]', '[BH3-]', '[SH]', '[SnH]', '[S@]', '[BH-]', '[S+]', '[PH2]', '7', '[OH-]', '[NH2+]', '[s+]', '[PH4]', '[Pt]', '[Cl-]', '[Zn]', '[n-]', '[Mg]', '[NH+]', '[Br-]', '[NH-]', '[B-]', '[Fe]', '[Pd]', '[Cl+3]', 'p', '[Pb]', '[SiH3]', '[I+]', '8', '9', '[N@+]', '[N@@+]', '[C]', '[N]', '[P@]', '[CH2-]', '[CH]', '[S@@+]', '[CH-]', '[S@@H]', '[O]', '[CH2]', '[P@@]', '[cH-]', '[S@+]', '[P@@H]', '[c-]', '[P@H]', '[F+]', '[N@@H+]', '[SH2]', '[11CH3]', '[P@@+]', '[o+]', '[S]', '[B@-]', '[SH3]', '[18F]', '[B@@H-]', '[125I]', '[124I]', '[P@+]', '[123I]', '[CH+]', '[BH2-]', '[18OH]', '[B@H-]', '%10', '[C+]', '[IH2]', '[O+]', '[Sn+2]', '[B@@-]', '[pH]', '[Br+]', '[17F]', '[3H]', '[SnH2]', '[Sn+3]', '<unk>', '<sos>', '<eos>', '<mask>', '<sum_pred>', '<sum_react>', '<0>', '<1>', '<2>', '<3>', '<pad>']\n",
      "83 ['\\\\', '[SiH3]', 'Cl', '2', 'Br', '[Mg+]', 'c', '[Se]', '[Zn+]', '[Mg]', '7', '[P+]', '[Zn]', '9', '[NH3+]', ')', '[Si]', '[C@@H]', '[C@H]', '[N+]', '[NH2+]', 'S', '%10', '/', 'I', 'B', 's', '6', '[se]', '.', '[SiH2]', '[n-]', '5', '[N@+]', '[N@@+]', '[PH2]', '[SnH]', 'o', '8', 'C', '3', '[n+]', '[SiH]', '[S@@]', '[S-]', 'p', '[NH4+]', '[PH4]', 'n', '[S+]', '[Sn]', '=', '[S@]', '[nH]', '#', '[N-]', '1', '[O-]', '[SH]', 'P', 'F', '(', '[C@]', '[Cu]', '[C@@]', '4', '[PH]', '-', '[s+]', 'O', '[C-]', 'N', '<unk>', '<sos>', '<eos>', '<mask>', '<sum_pred>', '<sum_react>', '<0>', '<1>', '<2>', '<3>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "with open('data/zinc/new_vocab.txt') as f:\n",
    "    zinc_vocab = [x.strip() for x in f.readlines()]\n",
    "print(len(zinc_vocab), zinc_vocab)\n",
    "\n",
    "with open('data/rooted/vocab.txt') as f:\n",
    "    rooted_vocab = [x.strip() for x in f.readlines()]\n",
    "print(len(rooted_vocab), rooted_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = rooted_vocab[-11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 72 72\n"
     ]
    }
   ],
   "source": [
    "set1 = set(zinc_vocab[:-11])\n",
    "set2 = set(rooted_vocab[:-11])\n",
    "print(len(set1), len(set2), len(set1 & set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vocab = set1.union(set2)\n",
    "final_vocab = list(final_vocab) + special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145 ['[N@@+]', '[S+]', '[125I]', '%10', '[SiH3]', '[18OH]', 'P', '[Fe]', '[B@@H-]', '[Cl-]', '[Cl+3]', '[BH3-]', '[Pb]', '[Se]', '[SH]', '[Mg]', '[SiH]', '[S@@]', 'c', 'F', 's', 'N', '[CH2-]', '/', '[3H]', '[C+]', '[O+]', '[P@H]', '[Br-]', 'O', '[C]', '[CH2]', '[Si]', '[B@@-]', '[NH+]', '[123I]', 'C', '[Pt]', '7', '[CH]', '[o+]', '[B@H-]', '[O-]', '[O]', '[BH2-]', '[I+]', '[N+]', '[Sn+2]', 'B', '[F+]', '[nH]', '-', '2', '[N-]', '9', '3', '4', 'n', '[Zn+]', '\\\\', '[P+]', '[C-]', '[S@@H]', '[P@]', '[NH4+]', 'Cl', '.', '[PH2]', 'S', '[s+]', 'p', '[124I]', '[C@@H]', '[SH2]', '[SH3]', '[SiH2]', '[11CH3]', '[pH]', '[Br+]', '[N@+]', '5', '[Li]', '[Cu]', '[S@@+]', '[K]', '[Sn]', '[SnH2]', '[NH2+]', '=', '[se]', '[SnH]', '[n+]', '[B@-]', '[S@]', '1', '[P@+]', '[c-]', '[S]', '[CH+]', '[Mg+]', '[C@@]', '[N@@H+]', '[cH-]', '[PH]', '6', '[B-]', '[OH-]', '[18F]', ')', '[PH4]', '[CH-]', '[BH-]', '[S-]', '[N]', 'I', '(', '8', '[S@+]', '[P@@]', '[Sn+3]', 'o', '[Zn]', '[C@]', '[P@@H]', '[NH-]', 'Br', '[P@@+]', '[Pd]', '[NH3+]', '[17F]', '[n-]', '#', '[IH2]', '[C@H]', '<unk>', '<sos>', '<eos>', '<mask>', '<sum_pred>', '<sum_react>', '<0>', '<1>', '<2>', '<3>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(len(final_vocab), final_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/final_vocab.txt', 'w') as f:\n",
    "    for item in final_vocab:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editDistDP(str1, str2, m, n):\n",
    "    # Create a table to store results of subproblems\n",
    "    dp = [[0 for x in range(n + 1)] for x in range(m + 1)]\n",
    " \n",
    "    # Fill d[][] in bottom up manner\n",
    "    for i in range(m + 1):\n",
    "        for j in range(n + 1):\n",
    " \n",
    "            # If first string is empty, only option is to\n",
    "            # insert all characters of second string\n",
    "            if i == 0:\n",
    "                dp[i][j] = j    # Min. operations = j\n",
    " \n",
    "            # If second string is empty, only option is to\n",
    "            # remove all characters of second string\n",
    "            elif j == 0:\n",
    "                dp[i][j] = i    # Min. operations = i\n",
    " \n",
    "            # If last characters are same, ignore last char\n",
    "            # and recur for remaining string\n",
    "            elif str1[i-1] == str2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    " \n",
    "            # If last character are different, consider all\n",
    "            # possibilities and find minimum\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i][j-1],        # Insert\n",
    "                                   dp[i-1][j],        # Remove\n",
    "                                   dp[i-1][j-1])    # Replace\n",
    "    # ans = dp[m][n]\n",
    "    return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_smiles = []\n",
    "p_smiles = []\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    with open(f'data/rooted/{split}/src-{split}.txt') as f:\n",
    "        src = f.read().splitlines()\n",
    "    with open(f'data/rooted/{split}/tgt-{split}.txt') as f:\n",
    "        tgt = f.read().splitlines()\n",
    "\n",
    "    for s in tqdm(src, desc=f'{split}'):\n",
    "        p_smiles.append(''.join(s.split()))\n",
    "    for t in tqdm(tgt, desc=f'{split}'):\n",
    "        r_smiles.append(''.join(t.split()))\n",
    "    # for t in tqdm(tgt, desc=f'{split}'):\n",
    "    #     for s_ind in ''.join(t.split()).split('.'):\n",
    "    #         smiles.append(s_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(r_smiles), r_smiles[3234:3238])\n",
    "print(len(p_smiles), p_smiles[3234:3238])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = editDistDP(r_smiles[3234], p_smiles[3234], len(r_smiles[3234]), len(p_smiles[3234]))\n",
    "dp[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(dp, xticklabels=list(p_smiles[3234]), yticklabels=list(r_smiles[3234]), cbar=False)\n",
    "plt.tick_params(axis='both', which='major', labelsize=10, labelbottom = False, bottom=False, top = False, labeltop=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vocab_size in ['', '100', '250', '750', '2000']:\n",
    "    vocab = set()\n",
    "\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        src_name = f'data/rooted/{split}/src-{split}_{vocab_size}.txt' if vocab_size != '' else f'data/rooted/{split}/src-{split}.txt'\n",
    "        tgt_name = f'data/rooted/{split}/tgt-{split}_{vocab_size}.txt' if vocab_size != '' else f'data/rooted/{split}/tgt-{split}.txt'\n",
    "        with open(src_name) as f:\n",
    "            src = f.read().splitlines()\n",
    "        with open(tgt_name) as f:\n",
    "            tgt = f.read().splitlines()\n",
    "\n",
    "        for s in tqdm(src, desc=f'{vocab_size}-{split}'):\n",
    "            vocab.update(s.split())\n",
    "        for t in tqdm(tgt, desc=f'{vocab_size}-{split}'):\n",
    "            vocab.update(t.split())\n",
    "\n",
    "    extra = ['<unk>', '<sos>', '<eos>', '<mask>', '<sum_pred>', '<sum_react>', '<0>', '<1>', '<2>', '<3>', '<pad>']\n",
    "    vocab = list(vocab) + extra\n",
    "\n",
    "    with open(f'data/rooted/vocab{vocab_size}.txt', 'w') as f:\n",
    "        f.write('\\n'.join(vocab))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab))\n",
    "print(vocab[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spe_vob = codecs.open('data/vocab_pairs/SPE_vocab_pairs_2000.txt')\n",
    "spe = SPE_Tokenizer(spe_vob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smi = ''.join('COC(=O) [C@H](C CCCN )N C(=O)N c1cc(OC )cc(C (C)(C)C)c1 O'.split())\n",
    "smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spe.tokenize(smi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zinc(Dataset):\n",
    "    def __init__(self, data_dir: str='/scratch/arihanth.srikar', split: str='train', to_gen: int=-1):\n",
    "        extra = ''\n",
    "        \n",
    "        # dataset files\n",
    "        # df = pd.read_pickle(f'{data_dir}/x001{extra}.pickle')\n",
    "        df = pd.read_csv(f'{data_dir}/x001.csv')\n",
    "        df = df[df['set'] == split].copy()\n",
    "        \n",
    "        # read entire dataset and convert to list\n",
    "        self.smiles = df['smiles'].tolist()\n",
    "        \n",
    "        # clear memory\n",
    "        del df\n",
    "        \n",
    "        # load specified number of samples\n",
    "        self.to_gen = to_gen if to_gen > 0 else len(self.smiles)\n",
    "        \n",
    "        # token encoder and decoder\n",
    "        with open(f'{data_dir}/vocab{extra}.txt', 'r') as f:\n",
    "            self.token_decoder = f.read().splitlines()\n",
    "        self.token_encoder = {k: v for v, k in enumerate(self.token_decoder)}\n",
    "\n",
    "        self.vocab_size = len(self.token_decoder)\n",
    "        self.pad_token_id = self.token_encoder['<pad>']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.to_gen\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # pick random indices if not utilizing entire dataset\n",
    "        if self.to_gen != len(self.smiles):\n",
    "            idx = torch.randint(0, len(self.smiles), (1,)).item()\n",
    "        \n",
    "        # treat the smiles as products\n",
    "        p = self.smiles[idx]\n",
    "        p = [self.token_encoder[tok] for tok in atomwise_tokenizer(p)]\n",
    "        \n",
    "        # append end of products token\n",
    "        p = [self.token_encoder['<sop>']] + p + [self.token_encoder['<eop>']]\n",
    "        mask = [1] * len(p)\n",
    "        \n",
    "        return torch.tensor(p), torch.tensor(mask)\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        smiles, mask = zip(*batch)\n",
    "        smiles = torch.nn.utils.rnn.pad_sequence(smiles, batch_first=True, padding_value=self.token_encoder['<pad>'])\n",
    "        mask = (smiles != self.token_encoder['<pad>']).bool()\n",
    "        return smiles, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Zinc(split='train', to_gen=100*384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=128, collate_fn=train_dataset.collate_fn, shuffle=True, num_workers=8, pin_memory=True, prefetch_factor=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles, mask = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_transformer = TransformerWrapper(\n",
    "    num_tokens = train_dataset.vocab_size,\n",
    "    max_seq_len = 512,\n",
    "    attn_layers = Encoder(\n",
    "        dim = 512,\n",
    "        depth = 6,\n",
    "        heads = 8,\n",
    "        rel_pos_bias = True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TransformerWrapper(\n",
    "    num_tokens = train_dataset.vocab_size,\n",
    "    max_seq_len = 512,\n",
    "    attn_layers = Decoder(\n",
    "        dim = 512,\n",
    "        depth = 6,\n",
    "        heads = 8,\n",
    "        rel_pos_bias = True,\n",
    "        cross_attend = True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = MLM(\n",
    "    encoder_transformer,\n",
    "    mask_token_id = train_dataset.token_encoder['<mask>'],          # the token id reserved for masking\n",
    "    pad_token_id = train_dataset.token_encoder['<pad>'],           # the token id for padding\n",
    "    mask_prob = 0.15,           # masking probability for masked language modeling\n",
    "    replace_prob = 0.90,        # ~10% probability that token will not be masked, but included in loss, as detailed in the epaper\n",
    "    mask_ignore_token_ids = []  # other tokens to exclude from masking, include the [cls] and [sep] here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = AutoregressiveWrapper(\n",
    "    decoder,\n",
    "    pad_value = train_dataset.token_encoder['<pad>'],\n",
    "    ignore_index=train_dataset.token_encoder['<pad>'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles, mask = next(iter(train_dataloader))\n",
    "smiles.shape, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.cuda()\n",
    "smiles, mask = smiles.cuda(), mask.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits, enc, loss = encoder(smiles, mask=mask, return_logits_and_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape, enc.shape, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoder_logits, decoder_loss = decoder(smiles, context=enc, context_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_logits.shape, decoder_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "optimizer = AdamW(list(encoder.parameters())+list(decoder.parameters()), lr=1e-4)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "for epoch in range(10):\n",
    "    avg_encoder_loss, avg_decoder_loss = 0, 0\n",
    "    with tqdm(train_dataloader) as pbar:\n",
    "        pbar.set_description(f'Epoch {epoch+1}')\n",
    "        for i, (smiles, mask) in enumerate(pbar):\n",
    "            smiles, mask = smiles.to(device), mask.to(device)\n",
    "            \n",
    "            encoder_logits, enc, encoder_loss = encoder(smiles, mask=mask, return_logits_and_embeddings=True)\n",
    "            decoder_logits, decoder_loss = decoder(smiles, context=enc, context_mask=mask)\n",
    "\n",
    "            encoder_loss.backward()\n",
    "            # decoder_loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            avg_encoder_loss += encoder_loss.item()\n",
    "            avg_decoder_loss += decoder_loss.item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'encoder_loss': encoder_loss.item(),\n",
    "                'decoder_loss': decoder_loss.item(),\n",
    "                'avg_encoder_loss': avg_encoder_loss/(i+1),\n",
    "                'avg_decoder_loss': avg_decoder_loss/(i+1)\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
