{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import get_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(f'/scratch/arihanth.srikar/data/zinc/x001.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, data_dir: str='/scratch/arihanth.srikar', split: str='train') -> None:\n",
    "\n",
    "        # data = pd.read_pickle(f'{data_dir}/data/zinc/zinc.pkl')\n",
    "        # data = pd.read_csv(f'{data_dir}/data/zinc/x001.csv')\n",
    "        # data = data[data['set'] == split]\n",
    "        self.smiles = data['smiles'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # get graph from smiles\n",
    "        x = get_graph(self.smiles[idx])\n",
    "        \n",
    "        # node features, positions, edge indices, edge features\n",
    "        node_feats = torch.tensor(x['node_feats'], dtype=torch.int64)  # N*9\n",
    "        positions  = torch.tensor(x['positions'], dtype=torch.float64) # N*3\n",
    "        edge_list  = torch.tensor(x['edge_index'], dtype=torch.int64)  # 2*E\n",
    "        edge_feats = torch.tensor(x['edge_attr'], dtype=torch.int64)   # E*3\n",
    "\n",
    "        # use 0 index for padding and prepare mask\n",
    "        node_feats = node_feats + 1 # 0 is reserved for padding\n",
    "        edge_feats = edge_feats + 1 # 0 is reserved for padding\n",
    "        mask = torch.ones(node_feats.size(0)).bool()\n",
    "\n",
    "        # construct adjacency matrix\n",
    "        row, col = edge_list\n",
    "        adj_mat = torch.zeros(row.size(0), col.size(0))\n",
    "        adj_mat[row, col] = 1\n",
    "        adj_mat[col, row] = 1\n",
    "        adj_mat[torch.arange(row.size(0)), torch.arange(row.size(0))] = 1\n",
    "\n",
    "        # contruct N*N*E dense edge features\n",
    "        dense_edges_feats = torch.zeros((edge_list.size(1), edge_list.size(1), edge_feats.size(1)), dtype=torch.int64)\n",
    "        dense_edges_feats[row, col, :] = edge_feats\n",
    "\n",
    "        return node_feats, positions, mask, adj_mat, dense_edges_feats\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "\n",
    "        # unpack the input data\n",
    "        node_feats, positions, mask, adj_mat, dense_edges_feats = zip(*data)\n",
    "        \n",
    "        # fina the largest graph in the batch\n",
    "        max_nodes = max([feats.size(0) for feats in node_feats])\n",
    "        \n",
    "        # pad the adjacency matrix, node features, positions with all 0s\n",
    "        adj_mat = torch.vstack([F.pad(mat, (0, max_nodes-mat.size(0), 0, max_nodes-mat.size(0)), \"constant\", 0).unsqueeze(0) for mat in adj_mat])\n",
    "        node_feats = pad_sequence(node_feats, batch_first=True, padding_value=0)\n",
    "        positions = pad_sequence(positions, batch_first=True, padding_value=0)\n",
    "        \n",
    "        # pad the mask with all False\n",
    "        mask = pad_sequence(mask, batch_first=True, padding_value=False)\n",
    "        \n",
    "        # pad each matrix in dense_edges_feats with all 0s\n",
    "        dense_edges_feats = torch.vstack([F.pad(mat, (0, 0, 0, max_nodes-mat.size(0), 0, max_nodes-mat.size(0)), \"constant\", 0).unsqueeze(0) for mat in dense_edges_feats])\n",
    "        \n",
    "        return node_feats, positions, mask, adj_mat, dense_edges_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zinc = GraphDataset(split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(Zinc, batch_size=1, collate_fn=Zinc.collate_fn, shuffle=True, num_workers=8, pin_memory=True, prefetch_factor=4)\n",
    "train_loader = DataLoader(Zinc, batch_size=12, collate_fn=Zinc.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feats, positions, mask, adj_mat, dense_edges_feats = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 30, 9]),\n",
       " torch.Size([12, 0]),\n",
       " torch.Size([12, 30]),\n",
       " torch.Size([12, 30, 30]),\n",
       " torch.Size([12, 30, 30, 3]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_feats.shape, positions.shape, mask.shape, adj_mat.shape, dense_edges_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../graph_transformer_pytorch/graph_transformer_pytorch/')\n",
    "from graph_transformer_pytorch import GraphTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_emb = nn.Linear(9, 64)\n",
    "edge_emb = nn.Linear(3, 64)\n",
    "model = GraphTransformer(\n",
    "    dim = 64,\n",
    "    depth = 2,\n",
    "    edge_dim = 64,             # optional - if left out, edge dimensions is assumed to be the same as the node dimensions above\n",
    "    with_feedforwards = True,   # whether to add a feedforward after each attention layer, suggested by literature to be needed\n",
    "    gated_residual = True,      # to use the gated residual to prevent over-smoothing\n",
    "    rel_pos_emb = True,          # set to True if the nodes are ordered, default to False\n",
    "    accept_adjacency_matrix = True  # set this to True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, edges = model(node_emb(node_feats.float()), edge_emb(dense_edges_feats.float()), adj_mat=adj_mat, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 30, 64]), torch.Size([12, 30, 30, 64]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.shape, edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
