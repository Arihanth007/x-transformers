{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from SmilesPE.pretokenizer import atomwise_tokenizer\n",
    "from SmilesPE.tokenizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levy.levenshteinaugment.levenshtein import Levenshtein_augment\n",
    "from rdkit import Chem\n",
    "\n",
    "#Supress warnings from RDKit\n",
    "from rdkit import rdBase\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Chemformer.molbart.util as util\n",
    "from Chemformer.molbart.data.datasets import ZincSlice\n",
    "from Chemformer.molbart.data.datamodules import MoleculeDataModule\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zinc(ZincSlice):\n",
    "    def __init__(self, data_path):\n",
    "        path = Path(data_path)\n",
    "\n",
    "        # If path is a directory then read every subfile\n",
    "        if path.is_dir():\n",
    "            df = self._read_dir_df(path)\n",
    "        else:\n",
    "            df = pd.read_csv(path)\n",
    "\n",
    "        super().__init__(df)\n",
    "\n",
    "    def _read_dir_df(self, path):\n",
    "        dfs = []\n",
    "        for f_name in tqdm(path.iterdir()):\n",
    "            df = pd.read_csv(f_name)\n",
    "            dfs.append(df)\n",
    "            break\n",
    "        zinc_df = pd.concat(dfs, ignore_index=True, copy=False)\n",
    "        return zinc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tokeniser...\n",
      "Finished tokeniser.\n"
     ]
    }
   ],
   "source": [
    "print(\"Building tokeniser...\")\n",
    "tokeniser = util.load_tokeniser('Chemformer/my_vocab.txt', 272)\n",
    "print(\"Finished tokeniser.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:12, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "zinc_dataset = Zinc('/scratch/arihanth.srikar/zinc/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building data module...\n",
      "Using a batch size of 8.\n",
      "Using molecule data module with augmentations.\n"
     ]
    }
   ],
   "source": [
    "print(\"Building data module...\")\n",
    "dm = MoleculeDataModule(\n",
    "        zinc_dataset,\n",
    "        tokeniser,\n",
    "        8,\n",
    "        512,\n",
    "        task='aug',\n",
    "        val_idxs=zinc_dataset.val_idxs,\n",
    "        test_idxs=zinc_dataset.test_idxs,\n",
    "        train_token_batch_size=None,\n",
    "        num_buckets=24,\n",
    "        unified_model=False,\n",
    "    )\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['encoder_input', 'encoder_pad_mask', 'decoder_input', 'decoder_pad_mask', 'target', 'target_mask', 'target_smiles'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([57, 8]), torch.Size([57, 8]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['encoder_input'].shape, sample['encoder_pad_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([52, 8]), torch.Size([52, 8]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['decoder_input'].shape, sample['decoder_pad_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([52, 8]), torch.Size([52, 8]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['target'].shape, sample['target_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sample['decoder_input'][1:, :] == sample['target'][:-1, :]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cn1cc(C[C@@H]2CCC[C@H]2NC(=O)NC[C@@]2(C)CCCNC2)cn1',\n",
       " 'Cn1cc(C(=O)NCc2ncc(-c3ccccc3)[nH]2)c(-c2cccnc2)n1',\n",
       " 'C[C@]1(Cc2cc(F)c(F)c(F)c2)CCCN1C(=O)NCc1ccc(C#N)c(F)c1',\n",
       " 'CN(CC(=O)N(C)C1CCCCC1)C[C@H](O)C[C@@]1(O)CCOC1',\n",
       " 'CC(C)(C)c1coc([C@H]2CCCN(Cc3ccnc(C4CCC4)n3)C2)n1',\n",
       " 'O=C(NCCNS(=O)(=O)c1cnn(CC2CC2)c1)c1ccccc1Cl',\n",
       " 'COc1cccc(OC(F)(F)F)c1CNC(=O)N[C@H](CCN)C(F)(F)F',\n",
       " 'Cc1ccc(C[C@@H](C)C(=O)NCC[C@@H]2CC(C)(C)CO2)cc1']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['target_smiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/uspto50/uspto_50.pickle')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import rdFMCS\n",
    "\n",
    "df_new = {\n",
    "    'reactants_mol': [],\n",
    "    'products_mol': [],\n",
    "    'reaction_type': [],\n",
    "    'set': [],\n",
    "    'importance': [],\n",
    "}\n",
    "\n",
    "for row in tqdm(df.itertuples(), total=len(df)):\n",
    "    product = row.products_mol\n",
    "    reactants = row.reactants_mol\n",
    "    reactants = Chem.MolToSmiles(reactants)\n",
    "    reactants = reactants.split('.')\n",
    "    reactants = [Chem.MolFromSmiles(reactant) for reactant in reactants]\n",
    "\n",
    "    # find overlap between reactant and product\n",
    "    sorted_reactants = []\n",
    "    for reactant in reactants:\n",
    "        overlap = rdFMCS.FindMCS([reactant, product])\n",
    "        sorted_reactants.append((reactant, overlap.numAtoms, overlap.numBonds))\n",
    "    sorted_reactants = sorted(sorted_reactants, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for i, (reactant, _, _) in enumerate(sorted_reactants):\n",
    "        df_new['reactants_mol'].append(reactant)\n",
    "        df_new['products_mol'].append(product)\n",
    "        df_new['reaction_type'].append(row.reaction_type)\n",
    "        df_new['set'].append(row.set)\n",
    "        df_new['importance'].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sike_df = pd.DataFrame(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sike_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sike_df['reactants_mol'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sike_df.to_pickle('data/uspto50/uspto_50_sike.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sike_df = pd.read_pickle('data/uspto50/uspto_50_sike.pickle')\n",
    "sike_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sike_df['IFT'] = sike_df['importance'].apply(lambda x: f'<IFT_{x+1}>')\n",
    "# sike_df.to_pickle('data/uspto50/uspto_50_sike.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sike_df.iloc[0]['reactants_mol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/uspto50/processed.pickle')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = Levenshtein_augment(source_augmentation=1, randomization_tries=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(reactants, products):\n",
    "    # reactants = sorted(reactants, key=lambda x: len(x), reverse=True)\n",
    "    # products  = sorted(products, key=lambda x: len(x), reverse=True)\n",
    "    \n",
    "    new_reactants, new_products, all_score = [], [], []\n",
    "    reactant, product = '.'.join(reactants), '.'.join(products)\n",
    "\n",
    "    pairs = augmenter.levenshtein_pairing(reactant, product)\n",
    "    augmentations = augmenter.sample_pairs(pairs)\n",
    "\n",
    "    for new_reactant, new_product, score in augmentations:\n",
    "        new_reactants.append(new_reactant)\n",
    "        new_products.append(new_product)\n",
    "        all_score.append(score)\n",
    "    \n",
    "    return new_reactants, new_products, all_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Chemformer/my_vocab.txt') as f:\n",
    "    char2idx = f.read().split('\\n')\n",
    "char2idx = {c: i for i, c in enumerate(char2idx)}\n",
    "idx2char = {i: c for i, c in enumerate(char2idx)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (reactants, products) in enumerate(tqdm(zip(df['reactants_mol'], df['products_mol']), total=len(df))):\n",
    "    new_products, new_reactants, score = augment(products, reactants)\n",
    "    print(f'{\".\".join(reactants)} -> {\".\".join(products)}')\n",
    "    for reactant, product, sc in zip(new_reactants, new_products, score):\n",
    "        print(f'{sc:.2f}: {reactant} -> {product}')\n",
    "        reactant = [char2idx['^']] + [char2idx[char] for char in atomwise_tokenizer(reactant)] + [char2idx['&']]\n",
    "        product  = [char2idx['^']] + [char2idx[char] for char in atomwise_tokenizer(product)] + [char2idx['&']]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "augment(df['products_mol'].iloc[0], df['reactants_mol'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LevySMILES(Dataset):\n",
    "    def __init__(self, split: str='val') -> None:\n",
    "\n",
    "        self.split = 'valid' if split == 'val' else split\n",
    "        \n",
    "        self.df = pd.read_pickle('data/uspto50/processed.pickle')\n",
    "        self.df = self.df[self.df['set'] == self.split]\n",
    "\n",
    "        self.augmenter = Levenshtein_augment(source_augmentation=1, randomization_tries=1000)\n",
    "\n",
    "        with open('Chemformer/my_vocab.txt') as f:\n",
    "            char2idx = f.read().split('\\n')\n",
    "        self.char2idx = {c: i for i, c in enumerate(char2idx)}\n",
    "        self.idx2char = {i: c for i, c in enumerate(char2idx)}\n",
    "        \n",
    "        self.start_token = self.char2idx['^']\n",
    "        self.end_token   = self.char2idx['&']\n",
    "        self.pad_token   = self.char2idx['<PAD>']\n",
    "\n",
    "    def augment(self, reactants, products):\n",
    "        new_reactants, new_products, all_score = [], [], []\n",
    "        reactant, product = '.'.join(reactants), '.'.join(products)\n",
    "\n",
    "        pairs = self.augmenter.levenshtein_pairing(reactant, product)\n",
    "        augmentations = self.augmenter.sample_pairs(pairs)\n",
    "\n",
    "        for new_reactant, new_product, score in augmentations:\n",
    "            new_reactants.append(new_reactant)\n",
    "            new_products.append(new_product)\n",
    "            all_score.append(score)\n",
    "        \n",
    "        return new_reactants, new_products, all_score\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        # get reactants and products\n",
    "        reactants, products = self.df['reactants_mol'].iloc[idx], self.df['products_mol'].iloc[idx]\n",
    "        \n",
    "        # augment and find best pair\n",
    "        new_products, new_reactants, score = self.augment(products, reactants)\n",
    "        new_reactants, new_products = new_reactants[0], new_products[0]\n",
    "        \n",
    "        # tokenize\n",
    "        new_reactants = [self.start_token] + [self.char2idx[char] for char in atomwise_tokenizer(new_reactants)] + [self.end_token]\n",
    "        new_products  = [self.start_token] + [self.char2idx[char] for char in atomwise_tokenizer(new_products)]  + [self.end_token]\n",
    "\n",
    "        # convert to tensor\n",
    "        new_reactants = torch.tensor(new_reactants).long()\n",
    "        new_products  = torch.tensor(new_products).long()\n",
    "        \n",
    "        return {'encoder_output': new_reactants, 'encoder_input': new_products}\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        # extract batch elements\n",
    "        encoder_input = [x['encoder_input'] for x in batch]\n",
    "        encoder_output = [x['encoder_output'] for x in batch]\n",
    "        \n",
    "        # pad to maximum length\n",
    "        encoder_input = torch.nn.utils.rnn.pad_sequence(encoder_input, batch_first=True, padding_value=self.pad_token)\n",
    "        encoder_output = torch.nn.utils.rnn.pad_sequence(encoder_output, batch_first=True, padding_value=self.pad_token)\n",
    "        \n",
    "        return {'encoder_input': encoder_input, 'encoder_output': encoder_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = LevySMILES(split='val')\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=16, collate_fn=val_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tqdm(val_loader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/uspto50/uspto_50_sike.pickle')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_df = df[df['importance'] == 0]\n",
    "single_df = single_df.reset_index(drop=True)\n",
    "single_df.to_pickle('data/uspto50/uspto_50_sike_single.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_df = pd.read_pickle('data/uspto50/uspto_50_sike_single.pickle')\n",
    "single_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
